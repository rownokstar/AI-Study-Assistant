{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPXMa+Q+yy6mTlK8NzqzBQO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rownokstar/AI-Study-Assistant/blob/main/AI_Study_Assistant.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2pSLb59fS7cb"
      },
      "outputs": [],
      "source": [
        "# Cell 1: Installation\n",
        "!pip install streamlit python-dotenv langchain langchain-community openai pypdf faiss-cpu pyngrok -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: Setup API Keys from Colab Secrets\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
        "os.environ[\"NGROK_AUTHTOKEN\"] = userdata.get('NGROK_AUTHTOKEN')"
      ],
      "metadata": {
        "id": "94086WE3S-7s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: Write the Final, Professional English Version of the App\n",
        "%%writefile app.py\n",
        "\n",
        "import streamlit as st\n",
        "import os\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.chains.summarize import load_summarize_chain\n",
        "\n",
        "# --- Core Functions ---\n",
        "\n",
        "def get_pdf_documents(pdf_files):\n",
        "    \"\"\"Loads, splits, and returns document chunks from uploaded PDF files.\"\"\"\n",
        "    documents = []\n",
        "    temp_dir = \"temp_pdf\"\n",
        "    if not os.path.exists(temp_dir):\n",
        "        os.makedirs(temp_dir)\n",
        "\n",
        "    for pdf_file in pdf_files:\n",
        "        file_path = os.path.join(temp_dir, pdf_file.name)\n",
        "        with open(file_path, \"wb\") as f:\n",
        "            f.write(pdf_file.getbuffer())\n",
        "\n",
        "        loader = PyPDFLoader(file_path)\n",
        "        documents.extend(loader.load())\n",
        "\n",
        "    text_splitter = CharacterTextSplitter(\n",
        "        separator=\"\\n\",\n",
        "        chunk_size=1000,\n",
        "        chunk_overlap=200,\n",
        "        length_function=len\n",
        "    )\n",
        "    document_chunks = text_splitter.split_documents(documents)\n",
        "    return document_chunks\n",
        "\n",
        "def get_vectorstore(document_chunks):\n",
        "    \"\"\"Creates a FAISS vector store from document chunks.\"\"\"\n",
        "    embeddings = OpenAIEmbeddings()\n",
        "    vectorstore = FAISS.from_documents(documents=document_chunks, embedding=embeddings)\n",
        "    return vectorstore\n",
        "\n",
        "def get_conversation_chain(vectorstore):\n",
        "    \"\"\"Creates a conversational retrieval chain.\"\"\"\n",
        "\n",
        "    llm = ChatOpenAI(temperature=0.3, max_tokens=500)\n",
        "    memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
        "    conversation_chain = ConversationalRetrievalChain.from_llm(\n",
        "        llm=llm,\n",
        "        retriever=vectorstore.as_retriever(),\n",
        "        memory=memory\n",
        "    )\n",
        "    return conversation_chain\n",
        "\n",
        "# --- Streamlit UI ---\n",
        "\n",
        "def main():\n",
        "    # --- Page Configuration ---\n",
        "    st.set_page_config(\n",
        "        page_title=\"AI Study Assistant\",\n",
        "        page_icon=\"ü§ñ\",\n",
        "        layout=\"wide\"\n",
        "    )\n",
        "\n",
        "    # --- Session State Initialization ---\n",
        "    if \"conversation\" not in st.session_state:\n",
        "        st.session_state.conversation = None\n",
        "    if \"chat_history\" not in st.session_state:\n",
        "        st.session_state.chat_history = []\n",
        "    if \"processed_documents\" not in st.session_state:\n",
        "        st.session_state.processed_documents = None\n",
        "    if \"summary\" not in st.session_state:\n",
        "        st.session_state.summary = None\n",
        "\n",
        "    # --- Header and Introduction ---\n",
        "    st.title(\"AI Study Assistant ü§ñ\")\n",
        "    st.write(\"Upload your textbooks or documents, and I'll help you study by answering questions and generating summaries.\")\n",
        "\n",
        "    with st.expander(\"‚ÑπÔ∏è How to Use\"):\n",
        "        st.markdown(\"\"\"\n",
        "        1.  **Upload:** Use the sidebar to upload one or more PDF documents.\n",
        "        2.  **Process:** Click the 'Process Documents' button to let the AI read them.\n",
        "        3.  **Ask:** Once processing is complete, ask any question about the content in the chat box below.\n",
        "        4.  **Summarize:** You can also generate a full summary of the documents using the button in the sidebar.\n",
        "        \"\"\")\n",
        "\n",
        "    st.markdown(\"---\")\n",
        "\n",
        "    # --- Chat Interface ---\n",
        "    st.header(\"Chat with Your Documents\")\n",
        "    user_question = st.text_input(\"Ask a question about the content of your documents:\")\n",
        "    if user_question and st.session_state.conversation:\n",
        "        response = st.session_state.conversation({'question': user_question})\n",
        "        st.session_state.chat_history = response['chat_history']\n",
        "\n",
        "    # Display chat history\n",
        "    for i, message in enumerate(st.session_state.chat_history):\n",
        "        if i % 2 == 0:\n",
        "            st.markdown(f\"<div style='text-align: right;'><b>You:</b> {message.content}</div>\", unsafe_allow_html=True)\n",
        "        else:\n",
        "            st.markdown(f\"<b>Bot:</b> {message.content}\")\n",
        "\n",
        "    # --- Sidebar for Controls ---\n",
        "    with st.sidebar:\n",
        "        st.header(\"Controls\")\n",
        "        st.subheader(\"1. Upload Documents\")\n",
        "        pdf_docs = st.file_uploader(\n",
        "            \"Upload your PDF files and click 'Process'\", accept_multiple_files=True, type=\"pdf\")\n",
        "\n",
        "        if st.button(\"Process Documents\"):\n",
        "            if pdf_docs:\n",
        "                with st.spinner(\"Processing documents... This may take a moment.\"):\n",
        "                    doc_chunks = get_pdf_documents(pdf_docs)\n",
        "                    st.session_state.processed_documents = doc_chunks\n",
        "\n",
        "                    vectorstore = get_vectorstore(doc_chunks)\n",
        "                    st.session_state.conversation = get_conversation_chain(vectorstore)\n",
        "                    st.success(\"Processing complete! You can now ask questions.\")\n",
        "            else:\n",
        "                st.error(\"Please upload at least one PDF file.\")\n",
        "\n",
        "        st.markdown(\"---\")\n",
        "\n",
        "        # --- Summarization Section ---\n",
        "        st.subheader(\"2. Additional Features\")\n",
        "        if st.button(\"Generate Full Summary\"):\n",
        "            if st.session_state.processed_documents:\n",
        "                with st.spinner(\"Generating summary... This can take several minutes for large documents.\"):\n",
        "                    llm = ChatOpenAI(temperature=0.2, model_name=\"gpt-3.5-turbo-16k\")\n",
        "                    summary_chain = load_summarize_chain(llm, chain_type=\"map_reduce\")\n",
        "                    summary = summary_chain.run(st.session_state.processed_documents)\n",
        "                    st.session_state.summary = summary\n",
        "            else:\n",
        "                st.warning(\"Please process your documents first to generate a summary.\")\n",
        "\n",
        "        if st.session_state.summary:\n",
        "            with st.expander(\"View Document Summary\"):\n",
        "                st.write(st.session_state.summary)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9DQe1ehOTK_R",
        "outputId": "dcc2b617-c56e-4f89-ccc2-90f262b0035b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4: Run the Streamlit app and expose it with ngrok\n",
        "!streamlit run app.py &>/dev/null&\n",
        "\n",
        "from pyngrok import ngrok\n",
        "import time\n",
        "\n",
        "# ngrok tunnel creation\n",
        "public_url = ngrok.connect(8501)\n",
        "print(\"‚úÖ The app is live now:\")\n",
        "print(public_url)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wlkudaWtTYqu",
        "outputId": "581d9a59-4c74-451c-a423-0069aa2f4acf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ The app is live now:\n",
            "NgrokTunnel: \"https://135f8589acf5.ngrok-free.app\" -> \"http://localhost:8501\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1xdtsFr1TeXz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}